{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f01b3f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-04T21:31:37.298703Z",
     "iopub.status.busy": "2025-07-04T21:31:37.298390Z",
     "iopub.status.idle": "2025-07-04T21:31:48.193498Z",
     "shell.execute_reply": "2025-07-04T21:31:48.192101Z"
    },
    "papermill": {
     "duration": 10.901971,
     "end_time": "2025-07-04T21:31:48.196488",
     "exception": false,
     "start_time": "2025-07-04T21:31:37.294517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/cmi-piu-data/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/cmi-piu-data/test.csv')\n",
    "data_dict = pd.read_csv('/kaggle/input/cmi-piu-data/data_dictionary.csv')\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e5d2ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T21:31:48.204623Z",
     "iopub.status.busy": "2025-07-04T21:31:48.204307Z",
     "iopub.status.idle": "2025-07-04T21:31:58.372069Z",
     "shell.execute_reply": "2025-07-04T21:31:58.370940Z"
    },
    "papermill": {
     "duration": 10.174118,
     "end_time": "2025-07-04T21:31:58.373804",
     "exception": false,
     "start_time": "2025-07-04T21:31:48.199686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (3960, 82)\n",
      "Test data shape: (20, 59)\n",
      "After cleaning - Training: (2736, 81), Test: (20, 59)\n",
      "\n",
      "=== MISSING DATA ANALYSIS ===\n",
      "Top 15 features with most missing data:\n",
      "                              Train_Missing_%  Test_Missing_%\n",
      "PAQ_A-Season                        86.732456            95.0\n",
      "PAQ_A-PAQ_A_Total                   86.732456            95.0\n",
      "Physical-Waist_Circumference        82.346491            75.0\n",
      "Fitness_Endurance-Time_Sec          73.391813            85.0\n",
      "Fitness_Endurance-Time_Mins         73.391813            85.0\n",
      "Fitness_Endurance-Max_Stage         73.282164            85.0\n",
      "FGC-FGC_GSND_Zone                   68.421053            75.0\n",
      "FGC-FGC_GSD_Zone                    68.421053            75.0\n",
      "FGC-FGC_GSD                         68.165205            75.0\n",
      "FGC-FGC_GSND                        68.128655            75.0\n",
      "Fitness_Endurance-Season            53.947368            80.0\n",
      "PAQ_C-Season                        47.368421            55.0\n",
      "PAQ_C-PAQ_C_Total                   47.368421            55.0\n",
      "BIA-BIA_TBW                         33.735380            60.0\n",
      "BIA-BIA_BMC                         33.735380            60.0\n",
      "\n",
      "Features with >80% missing data (3): ['PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'Physical-Waist_Circumference']\n",
      "\n",
      "=== FEATURE GROUPS ===\n",
      "demographics: 3 features\n",
      "physical_basic: 7 features\n",
      "fitness: 3 features\n",
      "functional_tests: 15 features\n",
      "body_composition: 17 features\n",
      "activity_questionnaires: 4 features\n",
      "psychological: 22 features\n",
      "other_assessments: 5 features\n",
      "education: 2 features\n",
      "\n",
      "=== FEATURE ENGINEERING ===\n",
      "Created 14 engineered features:\n",
      "['Physical-BMI_missing', 'PCIAT_Neglect', 'PAQ_A-PAQ_A_Total_missing', 'PCIAT_Compulsive', 'BMI_category', 'Fat_to_FFM_Ratio', 'Fitness_Index', 'PCIAT_Control', 'BMI_Age_Adjusted', 'Activity_Screen_Balance', 'BP_Category', 'Waist_Height_Ratio', 'PCIAT-PCIAT_Total_missing', 'Age_Group']\n",
      "\n",
      "=== MISSING VALUE IMPUTATION ===\n",
      "Missing value imputation completed!\n",
      "Remaining missing values in train: 0\n",
      "Remaining missing values in test: 0\n",
      "\n",
      "=== FEATURE SELECTION (mutual_info) ===\n",
      "Selected 40 features out of 95\n",
      "Top 10 selected features:\n",
      "  PCIAT-PCIAT_Total: 1.006\n",
      "  PCIAT_Control: 0.690\n",
      "  PCIAT_Compulsive: 0.540\n",
      "  PCIAT_Neglect: 0.456\n",
      "  PCIAT-PCIAT_05: 0.385\n",
      "  PCIAT-PCIAT_02: 0.371\n",
      "  PCIAT-PCIAT_15: 0.357\n",
      "  PCIAT-PCIAT_03: 0.349\n",
      "  PCIAT-PCIAT_17: 0.311\n",
      "  PCIAT-PCIAT_20: 0.300\n",
      "\n",
      "=== FEATURE SCALING (robust) ===\n",
      "Feature scaling completed!\n",
      "\n",
      "=== TRAINING BASELINE MODELS ===\n",
      "\n",
      "Training Ridge...\n",
      "  CV RMSE: 0.3518 (+/- 0.1791)\n",
      "\n",
      "Training Lasso...\n",
      "  CV RMSE: 0.3657 (+/- 0.0495)\n",
      "\n",
      "Training ElasticNet...\n",
      "  CV RMSE: 0.3453 (+/- 0.0617)\n",
      "\n",
      "Training RandomForest...\n",
      "  CV RMSE: 0.0010 (+/- 0.0013)\n",
      "\n",
      "Training GradientBoosting...\n",
      "  CV RMSE: 0.0000 (+/- 0.0000)\n",
      "\n",
      "=== TRAINING ADVANCED MODELS ===\n",
      "XGBoost CV RMSE: 0.0000 (+/- 0.0000)\n",
      "LightGBM CV RMSE: 0.0232 (+/- 0.0189)\n",
      "\n",
      "Best model: GradientBoosting\n",
      "Predictions shape: (20,)\n",
      "Preprocessing pipeline ready!\n",
      "Uncomment the lines in main() to run the full pipeline\n"
     ]
    }
   ],
   "source": [
    "class CMIPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline for CMI Problematic Internet Use dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.imputers = {}\n",
    "        self.label_encoders = {}\n",
    "        self.feature_names = []\n",
    "        self.engineered_features = []\n",
    "        \n",
    "    def load_and_initial_clean(self, train_path, test_path):\n",
    "        \"\"\"Load data and perform initial cleaning\"\"\"\n",
    "        self.train_df = pd.read_csv(\"/kaggle/input/cmi-piu-data/train.csv\")\n",
    "        self.test_df = pd.read_csv(\"/kaggle/input/cmi-piu-data/test.csv\")\n",
    "        \n",
    "        print(f\"Training data shape: {self.train_df.shape}\")\n",
    "        print(f\"Test data shape: {self.test_df.shape}\")\n",
    "        \n",
    "        # Remove completely empty rows\n",
    "        self.train_df = self.train_df.dropna(how='all')\n",
    "        self.test_df = self.test_df.dropna(how='all')\n",
    "\n",
    "        # ï¸Drop rows where the target is missing\n",
    "        self.train_df = self.train_df.dropna(subset=['sii'])\n",
    "        \n",
    "        # Identify target variable\n",
    "        self.target = 'sii'\n",
    "        \n",
    "        # Separate features and target\n",
    "        self.y_train = self.train_df[self.target].copy()\n",
    "\n",
    "        assert self.y_train.isnull().sum() == 0, \"Target contains missing values!\"\n",
    "        \n",
    "        self.X_train = self.train_df.drop(columns=[self.target]).copy()\n",
    "        self.X_test = self.test_df.copy()\n",
    "        \n",
    "        print(f\"After cleaning - Training: {self.X_train.shape}, Test: {self.X_test.shape}\")\n",
    "        return self\n",
    "    \n",
    "    def analyze_missing_data(self):\n",
    "        \"\"\"Analyze missing data patterns\"\"\"\n",
    "        print(\"\\n=== MISSING DATA ANALYSIS ===\")\n",
    "        \n",
    "        # Calculate missing percentages\n",
    "        train_missing = (self.X_train.isnull().sum() / len(self.X_train)) * 100\n",
    "        test_missing = (self.X_test.isnull().sum() / len(self.X_test)) * 100\n",
    "        \n",
    "        missing_df = pd.DataFrame({\n",
    "            'Train_Missing_%': train_missing,\n",
    "            'Test_Missing_%': test_missing\n",
    "        }).sort_values('Train_Missing_%', ascending=False)\n",
    "        \n",
    "        print(\"Top 15 features with most missing data:\")\n",
    "        print(missing_df.head(15))\n",
    "        \n",
    "        # Features with >80% missing\n",
    "        high_missing = missing_df[missing_df['Train_Missing_%'] > 80].index.tolist()\n",
    "        print(f\"\\nFeatures with >80% missing data ({len(high_missing)}): {high_missing}\")\n",
    "        \n",
    "        return missing_df\n",
    "    \n",
    "    def create_feature_groups(self):\n",
    "        \"\"\"Group related features for better handling\"\"\"\n",
    "        self.feature_groups = {\n",
    "            'demographics': ['Basic_Demos-Age', 'Basic_Demos-Sex', 'Basic_Demos-Enroll_Season'],\n",
    "            'physical_basic': ['Physical-BMI', 'Physical-Height', 'Physical-Weight', \n",
    "                             'Physical-Waist_Circumference', 'Physical-Diastolic_BP', \n",
    "                             'Physical-HeartRate', 'Physical-Systolic_BP'],\n",
    "            'fitness': ['Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', \n",
    "                       'Fitness_Endurance-Time_Sec'],\n",
    "            'functional_tests': [col for col in self.X_train.columns if 'FGC-' in col],\n",
    "            'body_composition': [col for col in self.X_train.columns if 'BIA-' in col],\n",
    "            'activity_questionnaires': [col for col in self.X_train.columns if 'PAQ_' in col],\n",
    "            'psychological': [col for col in self.X_train.columns if 'PCIAT-' in col],\n",
    "            'other_assessments': [col for col in self.X_train.columns if 'SDS-' in col or 'CGAS-' in col],\n",
    "            'education': [col for col in self.X_train.columns if 'PreInt_EduHx-' in col]\n",
    "        }\n",
    "        \n",
    "        print(\"\\n=== FEATURE GROUPS ===\")\n",
    "        for group, features in self.feature_groups.items():\n",
    "            print(f\"{group}: {len(features)} features\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def engineer_features(self):\n",
    "        \"\"\"Create engineered features\"\"\"\n",
    "        print(\"\\n=== FEATURE ENGINEERING ===\")\n",
    "        \n",
    "        # Combine train and test for consistent feature engineering\n",
    "        combined_df = pd.concat([self.X_train, self.X_test], ignore_index=True)\n",
    "        \n",
    "        # 1. Physical Health Composite Features\n",
    "        if 'Physical-BMI' in combined_df.columns:\n",
    "            # BMI categories\n",
    "            combined_df['BMI_category'] = pd.cut(combined_df['Physical-BMI'], \n",
    "                                               bins=[0, 18.5, 25, 30, 100], \n",
    "                                               labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
    "            \n",
    "            # Waist-to-height ratio (if both available)\n",
    "            if 'Physical-Waist_Circumference' in combined_df.columns and 'Physical-Height' in combined_df.columns:\n",
    "                combined_df['Waist_Height_Ratio'] = combined_df['Physical-Waist_Circumference'] / combined_df['Physical-Height']\n",
    "        \n",
    "        # 2. Blood Pressure Categories\n",
    "        if 'Physical-Systolic_BP' in combined_df.columns and 'Physical-Diastolic_BP' in combined_df.columns:\n",
    "            combined_df['BP_Category'] = 'Normal'\n",
    "            combined_df.loc[(combined_df['Physical-Systolic_BP'] >= 140) | \n",
    "                           (combined_df['Physical-Diastolic_BP'] >= 90), 'BP_Category'] = 'High'\n",
    "            combined_df.loc[(combined_df['Physical-Systolic_BP'] >= 120) & \n",
    "                           (combined_df['Physical-Systolic_BP'] < 140), 'BP_Category'] = 'Elevated'\n",
    "        \n",
    "        # 3. Fitness Performance Index\n",
    "        fitness_cols = ['Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins']\n",
    "        if all(col in combined_df.columns for col in fitness_cols):\n",
    "            # Normalize and combine fitness metrics\n",
    "            combined_df['Fitness_Index'] = (\n",
    "                combined_df['Fitness_Endurance-Max_Stage'].fillna(0) * 0.6 +\n",
    "                combined_df['Fitness_Endurance-Time_Mins'].fillna(0) * 0.4\n",
    "            )\n",
    "        \n",
    "        # 4. PCIAT Subscales (if PCIAT items available)\n",
    "        pciat_cols = [col for col in combined_df.columns if 'PCIAT-PCIAT_' in col and col != 'PCIAT-PCIAT_Total']\n",
    "        if len(pciat_cols) >= 10:\n",
    "            # Create subscales based on common PCIAT factor structure\n",
    "            # Compulsive Use (items 1, 2, 3, 4, 5)\n",
    "            compulsive_items = [f'PCIAT-PCIAT_{i:02d}' for i in range(1, 6) if f'PCIAT-PCIAT_{i:02d}' in combined_df.columns]\n",
    "            if compulsive_items:\n",
    "                combined_df['PCIAT_Compulsive'] = combined_df[compulsive_items].mean(axis=1)\n",
    "            \n",
    "            # Neglect (items 6, 7, 8, 9, 10)\n",
    "            neglect_items = [f'PCIAT-PCIAT_{i:02d}' for i in range(6, 11) if f'PCIAT-PCIAT_{i:02d}' in combined_df.columns]\n",
    "            if neglect_items:\n",
    "                combined_df['PCIAT_Neglect'] = combined_df[neglect_items].mean(axis=1)\n",
    "            \n",
    "            # Control (items 11-20)\n",
    "            control_items = [f'PCIAT-PCIAT_{i:02d}' for i in range(11, 21) if f'PCIAT-PCIAT_{i:02d}' in combined_df.columns]\n",
    "            if control_items:\n",
    "                combined_df['PCIAT_Control'] = combined_df[control_items].mean(axis=1)\n",
    "        \n",
    "        # 5. Age-adjusted features\n",
    "        if 'Basic_Demos-Age' in combined_df.columns:\n",
    "            # Age groups\n",
    "            combined_df['Age_Group'] = pd.cut(combined_df['Basic_Demos-Age'], \n",
    "                                           bins=[0, 8, 12, 16, 25], \n",
    "                                           labels=['Child', 'PreTeen', 'Teen', 'Young_Adult'])\n",
    "            \n",
    "            # Age-adjusted BMI percentiles (simplified)\n",
    "            if 'Physical-BMI' in combined_df.columns:\n",
    "                combined_df['BMI_Age_Adjusted'] = combined_df['Physical-BMI'] / (combined_df['Basic_Demos-Age'] / 10)\n",
    "        \n",
    "        # 6. Body Composition Ratios\n",
    "        if 'BIA-BIA_Fat' in combined_df.columns and 'BIA-BIA_FFM' in combined_df.columns:\n",
    "            combined_df['Fat_to_FFM_Ratio'] = combined_df['BIA-BIA_Fat'] / (combined_df['BIA-BIA_FFM'] + 1e-6)\n",
    "        \n",
    "        # 7. Activity vs Screen Time Balance\n",
    "        if 'PAQ_A-PAQ_A_Total' in combined_df.columns and 'PreInt_EduHx-computerinternet_hoursday' in combined_df.columns:\n",
    "            combined_df['Activity_Screen_Balance'] = combined_df['PAQ_A-PAQ_A_Total'] / (combined_df['PreInt_EduHx-computerinternet_hoursday'] + 1e-6)\n",
    "        \n",
    "        # 8. Missingness indicators for important features\n",
    "        important_features = ['Physical-BMI', 'PCIAT-PCIAT_Total', 'PAQ_A-PAQ_A_Total']\n",
    "        for feature in important_features:\n",
    "            if feature in combined_df.columns:\n",
    "                combined_df[f'{feature}_missing'] = combined_df[feature].isnull().astype(int)\n",
    "        \n",
    "        # Split back to train and test\n",
    "        self.X_train_engineered = combined_df.iloc[:len(self.X_train)].copy()\n",
    "        self.X_test_engineered = combined_df.iloc[len(self.X_train):].copy()\n",
    "        \n",
    "        # Track engineered features\n",
    "        original_features = set(self.X_train.columns)\n",
    "        new_features = set(self.X_train_engineered.columns) - original_features\n",
    "        self.engineered_features = list(new_features)\n",
    "        \n",
    "        print(f\"Created {len(self.engineered_features)} engineered features:\")\n",
    "        print(self.engineered_features)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"Smart missing value imputation\"\"\"\n",
    "        print(\"\\n=== MISSING VALUE IMPUTATION ===\")\n",
    "        \n",
    "        # Separate numerical and categorical features\n",
    "        numerical_features = self.X_train_engineered.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = self.X_train_engineered.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Handle categorical features\n",
    "        for col in categorical_features:\n",
    "            if col in self.X_train_engineered.columns:\n",
    "                # Use mode for categorical\n",
    "                mode_val = self.X_train_engineered[col].mode()[0] if not self.X_train_engineered[col].mode().empty else 'Unknown'\n",
    "                self.X_train_engineered[col] = self.X_train_engineered[col].fillna(mode_val)\n",
    "                self.X_test_engineered[col] = self.X_test_engineered[col].fillna(mode_val)\n",
    "                \n",
    "                # Label encode categorical features\n",
    "                le = LabelEncoder()\n",
    "                combined_values = pd.concat([self.X_train_engineered[col], self.X_test_engineered[col]])\n",
    "                le.fit(combined_values)\n",
    "                self.X_train_engineered[col] = le.transform(self.X_train_engineered[col])\n",
    "                self.X_test_engineered[col] = le.transform(self.X_test_engineered[col])\n",
    "                self.label_encoders[col] = le\n",
    "        \n",
    "        # Handle numerical features with different strategies\n",
    "        # 1. Simple imputation for basic features\n",
    "        basic_features = ['Basic_Demos-Age', 'Physical-BMI', 'Physical-Height', 'Physical-Weight']\n",
    "        basic_numerical = [col for col in basic_features if col in numerical_features]\n",
    "        \n",
    "        if basic_numerical:\n",
    "            imputer_basic = SimpleImputer(strategy='median')\n",
    "            self.X_train_engineered[basic_numerical] = imputer_basic.fit_transform(self.X_train_engineered[basic_numerical])\n",
    "            self.X_test_engineered[basic_numerical] = imputer_basic.transform(self.X_test_engineered[basic_numerical])\n",
    "            self.imputers['basic'] = imputer_basic\n",
    "        \n",
    "        # 2. KNN imputation for correlated features (body composition)\n",
    "        bia_features = [col for col in numerical_features if 'BIA-' in col]\n",
    "        if len(bia_features) > 3:\n",
    "            imputer_knn = KNNImputer(n_neighbors=5)\n",
    "            self.X_train_engineered[bia_features] = imputer_knn.fit_transform(self.X_train_engineered[bia_features])\n",
    "            self.X_test_engineered[bia_features] = imputer_knn.transform(self.X_test_engineered[bia_features])\n",
    "            self.imputers['bia'] = imputer_knn\n",
    "        \n",
    "        # 3. Zero imputation for questionnaire items (assuming missing = not endorsed)\n",
    "        questionnaire_features = [col for col in numerical_features if any(x in col for x in ['PCIAT-PCIAT_', 'PAQ_', 'SDS-'])]\n",
    "        if questionnaire_features:\n",
    "            self.X_train_engineered[questionnaire_features] = self.X_train_engineered[questionnaire_features].fillna(0)\n",
    "            self.X_test_engineered[questionnaire_features] = self.X_test_engineered[questionnaire_features].fillna(0)\n",
    "        \n",
    "        # 4. Median imputation for remaining features\n",
    "        remaining_features = [col for col in numerical_features \n",
    "                            if col not in basic_numerical + bia_features + questionnaire_features]\n",
    "        if remaining_features:\n",
    "            imputer_remaining = SimpleImputer(strategy='median')\n",
    "            self.X_train_engineered[remaining_features] = imputer_remaining.fit_transform(self.X_train_engineered[remaining_features])\n",
    "            self.X_test_engineered[remaining_features] = imputer_remaining.transform(self.X_test_engineered[remaining_features])\n",
    "            self.imputers['remaining'] = imputer_remaining\n",
    "        \n",
    "        print(\"Missing value imputation completed!\")\n",
    "        print(f\"Remaining missing values in train: {self.X_train_engineered.isnull().sum().sum()}\")\n",
    "        print(f\"Remaining missing values in test: {self.X_test_engineered.isnull().sum().sum()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def feature_selection(self, method='mutual_info', k=40):\n",
    "        \"\"\"Select most relevant features\"\"\"\n",
    "        print(f\"\\n=== FEATURE SELECTION ({method}) ===\")\n",
    "        \n",
    "        if method == 'mutual_info':\n",
    "            selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
    "        else:\n",
    "            selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        \n",
    "        self.X_train_selected = selector.fit_transform(self.X_train_engineered, self.y_train)\n",
    "        self.X_test_selected = selector.transform(self.X_test_engineered)\n",
    "        \n",
    "        # Get selected feature names\n",
    "        selected_features = self.X_train_engineered.columns[selector.get_support()].tolist()\n",
    "        self.selected_features = selected_features\n",
    "        \n",
    "        print(f\"Selected {len(selected_features)} features out of {self.X_train_engineered.shape[1]}\")\n",
    "        print(\"Top 10 selected features:\")\n",
    "        feature_scores = selector.scores_[selector.get_support()]\n",
    "        top_features = sorted(zip(selected_features, feature_scores), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for feature, score in top_features:\n",
    "            print(f\"  {feature}: {score:.3f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def scale_features(self, method='robust'):\n",
    "        \"\"\"Scale features for modeling\"\"\"\n",
    "        print(f\"\\n=== FEATURE SCALING ({method}) ===\")\n",
    "        \n",
    "        if method == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "        \n",
    "        self.X_train_scaled = scaler.fit_transform(self.X_train_selected)\n",
    "        self.X_test_scaled = scaler.transform(self.X_test_selected)\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        print(\"Feature scaling completed!\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_processed_data(self):\n",
    "        \"\"\"Return processed data for modeling\"\"\"\n",
    "        return {\n",
    "            'X_train': self.X_train_scaled,\n",
    "            'X_test': self.X_test_scaled,\n",
    "            'y_train': self.y_train,\n",
    "            'feature_names': self.selected_features,\n",
    "            'original_train_df': self.train_df,\n",
    "            'original_test_df': self.test_df\n",
    "        }\n",
    "\n",
    "# Model Training and Evaluation Class\n",
    "class CMIModelTrainer:\n",
    "    \"\"\"\n",
    "    Model training and evaluation for CMI dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_test, feature_names):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.feature_names = feature_names\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def train_baseline_models(self):\n",
    "        \"\"\"Train baseline models\"\"\"\n",
    "        print(\"\\n=== TRAINING BASELINE MODELS ===\")\n",
    "        \n",
    "        # Define models\n",
    "        models = {\n",
    "            'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "            'Lasso': Lasso(alpha=0.1, random_state=42),\n",
    "            'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42),\n",
    "            'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "            'GradientBoosting': GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42)\n",
    "        }\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            # Fit model\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            self.models[name] = model\n",
    "            \n",
    "            # Cross-validation scores\n",
    "            cv_scores = cross_val_score(model, self.X_train, self.y_train, \n",
    "                                      cv=5, scoring='neg_mean_squared_error')\n",
    "            \n",
    "            # Store results\n",
    "            self.results[name] = {\n",
    "                'cv_rmse_mean': np.sqrt(-cv_scores.mean()),\n",
    "                'cv_rmse_std': np.sqrt(cv_scores.std()),\n",
    "                'cv_scores': cv_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"  CV RMSE: {self.results[name]['cv_rmse_mean']:.4f} (+/- {self.results[name]['cv_rmse_std']:.4f})\")\n",
    "    \n",
    "    def train_advanced_models(self):\n",
    "        \"\"\"Train advanced models (XGBoost, LightGBM)\"\"\"\n",
    "        print(\"\\n=== TRAINING ADVANCED MODELS ===\")\n",
    "        \n",
    "        # XGBoost\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        xgb_model.fit(self.X_train, self.y_train)\n",
    "        self.models['XGBoost'] = xgb_model\n",
    "        \n",
    "        cv_scores = cross_val_score(xgb_model, self.X_train, self.y_train, \n",
    "                                  cv=5, scoring='neg_mean_squared_error')\n",
    "        self.results['XGBoost'] = {\n",
    "            'cv_rmse_mean': np.sqrt(-cv_scores.mean()),\n",
    "            'cv_rmse_std': np.sqrt(cv_scores.std()),\n",
    "            'cv_scores': cv_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"XGBoost CV RMSE: {self.results['XGBoost']['cv_rmse_mean']:.4f} (+/- {self.results['XGBoost']['cv_rmse_std']:.4f})\")\n",
    "        \n",
    "        # LightGBM\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            verbosity=-1\n",
    "        )\n",
    "        \n",
    "        lgb_model.fit(self.X_train, self.y_train)\n",
    "        self.models['LightGBM'] = lgb_model\n",
    "        \n",
    "        cv_scores = cross_val_score(lgb_model, self.X_train, self.y_train, \n",
    "                                  cv=5, scoring='neg_mean_squared_error')\n",
    "        self.results['LightGBM'] = {\n",
    "            'cv_rmse_mean': np.sqrt(-cv_scores.mean()),\n",
    "            'cv_rmse_std': np.sqrt(cv_scores.std()),\n",
    "            'cv_scores': cv_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"LightGBM CV RMSE: {self.results['LightGBM']['cv_rmse_mean']:.4f} (+/- {self.results['LightGBM']['cv_rmse_std']:.4f})\")\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Get the best performing model\"\"\"\n",
    "        best_model_name = min(self.results.keys(), key=lambda x: self.results[x]['cv_rmse_mean'])\n",
    "        return best_model_name, self.models[best_model_name]\n",
    "    \n",
    "    def generate_predictions(self, model_name=None):\n",
    "        \"\"\"Generate predictions for test set\"\"\"\n",
    "        if model_name is None:\n",
    "            model_name, _ = self.get_best_model()\n",
    "        \n",
    "        model = self.models[model_name]\n",
    "        predictions = model.predict(self.X_test)\n",
    "        \n",
    "        return predictions, model_name\n",
    "\n",
    "# Usage Example\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main preprocessing and modeling pipeline\n",
    "    \"\"\"\n",
    "    # Initialize preprocessor\n",
    "    preprocessor = CMIPreprocessor()\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    preprocessor.load_and_initial_clean('train.csv', 'test.csv')\n",
    "    preprocessor.analyze_missing_data()\n",
    "    preprocessor.create_feature_groups()\n",
    "    preprocessor.engineer_features()\n",
    "    preprocessor.handle_missing_values()\n",
    "    preprocessor.feature_selection(method='mutual_info', k=40)\n",
    "    preprocessor.scale_features(method='robust')\n",
    "    \n",
    "    # Get processed data\n",
    "    data = preprocessor.get_processed_data()\n",
    "    \n",
    "    # Initialize model trainer\n",
    "    trainer = CMIModelTrainer(data['X_train'], data['y_train'], \n",
    "                            data['X_test'], data['feature_names'])\n",
    "    \n",
    "    # Train models\n",
    "    trainer.train_baseline_models()\n",
    "    trainer.train_advanced_models()\n",
    "    \n",
    "    # Get best model and predictions\n",
    "    best_model_name, best_model = trainer.get_best_model()\n",
    "    predictions, model_name = trainer.generate_predictions()\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    print(\"Preprocessing pipeline ready!\")\n",
    "    print(\"Uncomment the lines in main() to run the full pipeline\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7803330,
     "sourceId": 12375679,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 198686398,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 203489307,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.889364,
   "end_time": "2025-07-04T21:31:59.398280",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-04T21:31:32.508916",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
