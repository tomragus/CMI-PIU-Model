{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2beb8aa7",
   "metadata": {},
   "source": [
    "# **Child Mind Institute - Relating Physical Activity to Problematic Internet Use**\n",
    "\n",
    "‚Ä£ GitHub page for this project üëâ  [here]()\n",
    "\n",
    "‚Ä£ An article from the institute on Summer Screen Time Use üëâ  [read here](https://childmind.org/article/screen-time-and-summer/)\n",
    "\n",
    "\n",
    "\n",
    "### The Problem at Hand üßë‚Äçüíª *(taken from the comptetition homepage üëâ  [read here](https://www.kaggle.com/competitions/child-mind-institute-problematic-internet-use/overview))*\n",
    "\n",
    "\"In today‚Äôs digital age, problematic internet use among children and adolescents is a growing concern. Better understanding this issue is crucial for addressing mental health problems such as depression and anxiety.\n",
    "\n",
    "Current methods for measuring problematic internet use in children and adolescents are often complex and require professional assessments. This creates access, cultural, and linguistic barriers for many families. Due to these limitations, problematic internet use is often not measured directly, but is instead associated with issues such as depression and anxiety in youth.\n",
    "\n",
    "Conversely, physical & fitness measures are extremely accessible and widely available with minimal intervention or clinical expertise. Changes in physical habits, such as poorer posture, irregular diet, and reduced physical activity, are common in excessive technology users. We propose using these easily obtainable physical fitness indicators as proxies for identifying problematic internet use, especially in contexts lacking clinical expertise or suitable assessment tools.\"\n",
    "\n",
    "**What does this mean?** The Child Mind Institute has tasked the public with building predictive machine learning models that will determine a participant's Severity Impairment Index (SII), which is a metric measuring the level of problematic internet use among children and adolescents, based on physical activity, health, and lifestyle factors. The aim is to identify signs of problematic internet use early so that preventative measures can be taken by the parent/ caretaker.\n",
    "\n",
    "### The Data at Hand üìä\n",
    "\n",
    "We will be working with the Child Mind Institute's *Healthy Brain Network (HBN)* dataset, a clinical sample of roughly 5,000 youth and adolescents (aged 5-22) that have undergone various clinical and research screenings for the institute. The institute has conveniently separated for us the relevant data into two distinct categories.\n",
    "\n",
    "The first is tabular data comprising measurements from various instruments, assessments, and questionairres - in particular, it includes an assessment called the Parent-Child Internet Addiction Test (PCIAT), which is used to calculate the SII of each participant - we'll refer to this data as **feature data**. The second is time-series data collected with a wrist accelerometer given to roughly 1,000 participants to wear for up to 30 days continually while at home and going about their daily lives. The data collected from this device includes physical activity and other metrics - we'll refer to this data as **actigraphy data**.\n",
    "\n",
    "For each we have been provided with a **train set**, on which we will train our models, and a **test set**, on which we will evaluate their performance. The train set is a full dataset that includes the SII, which is our **target variable**, and the PCIAT results used to calculate it - the test set is a much smaller collection of data that is missing this information. Our objective then is to train the models to accurately predict SII values *for each entry in the test set*.\n",
    "\n",
    "Because the natures of the feature data and the actigraphy data are vastly different, we will use an **ensemble approach**, analyzing, feature engineering and training models separately, then merging results for the final submission. The actigraphy data is dense time-series data, which means we will get a lot of value from training a neural network on it. For the feature data, simpler baseline ML models should be appropriate. \n",
    "\n",
    "### Competition Evaluation üìù\n",
    "\n",
    "The result will be evaluated based on the **quadratic weighted kappa**, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). The submission file will consist of two rows, one for id and one for SII, with an entry for each participant in the test set. An example submission has been given to us on the Kaggle page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e41a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas\n",
    "import pandas as pd\n",
    "\n",
    "# load sample submission\n",
    "sample = pd.read_csv(\"/Users/tomragus/Library/CloudStorage/OneDrive-UCSanDiego/CMI-PIU-Model/data/sample_submission.csv\")\n",
    "\n",
    "# display sample submission\n",
    "print(\"Sample submission\")\n",
    "print(f\"Submission shape: {sample.shape}\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10819b19",
   "metadata": {},
   "source": [
    "### Credit üìö\n",
    "\n",
    "Parts of this notebook, particularly in the EDA stage, were adapted from [Antonina Dolgorukova](https://datadelic.dev/)'s brilliant EDA notebooks for this competition. I highly encourage checking out her work - they are extremely in-depth and very well written.\n",
    "\n",
    "‚Ä£ *Feature EDA üëâ  [read here](https://www.kaggle.com/code/antoninadolgorukova/cmi-piu-features-eda/notebook)*\n",
    "\n",
    "‚Ä£ *Actigraphy EDA üëâ  [read here](https://www.kaggle.com/code/antoninadolgorukova/cmi-piu-actigraphy-data-eda)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfb144",
   "metadata": {},
   "source": [
    "\n",
    "## ***Feature data***\n",
    "\n",
    "Let's start by taking a peek into our feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train set\n",
    "train = pd.read_csv(\"/Users/tomragus/Library/CloudStorage/OneDrive-UCSanDiego/CMI-PIU-Model/data/train.csv\")\n",
    "\n",
    "# display first 5 rows of train set\n",
    "print(\"\"\"Train set: where the 'features' live\"\"\")\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "test = pd.read_csv(\"/Users/tomragus/Library/CloudStorage/OneDrive-UCSanDiego/CMI-PIU-Model/data/test.csv\")\n",
    "\n",
    "# display first 5 rows of test set\n",
    "print(\"\"\"Test set: what we will evaluate our models on\"\"\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c402eb",
   "metadata": {},
   "source": [
    "It can be tricky to figure out what all of these abbreviations mean - thankfully, the Child Mind Institute was kind enough to include a **data dictionary** for this competition, which gives some extra information for each variable. Here is a little preview - [you can view the full file on the Kaggle page](https://www.kaggle.com/competitions/child-mind-institute-problematic-internet-use/data?select=data_dictionary.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adad4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data dictionary\n",
    "data_dict = pd.read_csv(\"/Users/tomragus/Library/CloudStorage/OneDrive-UCSanDiego/CMI-PIU-Model/data/data_dictionary.csv\")\n",
    "\n",
    "# display first 5 rows of data dictionary\n",
    "print(\"\"\"Data Dictionary: what each feature means\"\"\")\n",
    "print(f\"Data Dictionary shape: {data_dict.shape}\")\n",
    "display(data_dict.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c7983",
   "metadata": {},
   "source": [
    "While this only gives us a snippet of the data at hand, we can see the SII and the PCIAT scores on the right side of the train set. The SII scores range from 0 to 3, with 0 representing no impairment, and 3 representing severe impairment. So, we can think of the problem as training our models to **classify** each id in the test set into one of the 4 SII classes (0, 1, 2 or 3). Classification calls for **supervised learning**.\n",
    "\n",
    "Looking at the shape of the train set, we can see that the train set has almost 4,000 entries ü§Ø this is good - the more data we have, the more finely we can tune our models.\n",
    "\n",
    "### Loading the rest of our libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all libraries\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from pandas.api.types import is_numeric_dtype, is_object_dtype, is_categorical_dtype, CategoricalDtype\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from datetime import datetime\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b0aaa6",
   "metadata": {},
   "source": [
    "### **Feature Data EDA**\n",
    "\n",
    "Let us first analyze observe the features that are related to the SII and are not present in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5484df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolating train-only features\n",
    "train_cols = set(train.columns)\n",
    "test_cols = set(test.columns)\n",
    "columns_not_in_test = sorted(list(train_cols - test_cols))\n",
    "\n",
    "# addind additional information using data dictionary\n",
    "data_dict[data_dict['Field'].isin(columns_not_in_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aeda00",
   "metadata": {},
   "source": [
    "Here we see each item in the Parent-Child Internet Addiction Test (PCIAT). Each question (third column) assesses a different aspect of a child's behavior related to internet use, and responses are given on a scale from 0 to 5 with the total score providing an indication of the severity of internet addiction.\n",
    "\n",
    "We also have the season of participation in PCIAT-Season and total score in PCIAT-PCIAT_Total; so there are a total of 22 PCIAT test-related columns.\n",
    "\n",
    "Now we will verify that the PCIAT-PCIAT_Total aligns with the corresponding SII categories by calculating the minimum and maximum scores for each SII category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate max and min\n",
    "pciat_min_max = train.groupby('sii')['PCIAT-PCIAT_Total'].agg(['min', 'max'])\n",
    "pciat_min_max = pciat_min_max.rename(columns={'min': 'Minimum PCIAT total Score', 'max': 'Maximum total PCIAT Score'})\n",
    "pciat_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd83363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display range for each level of severity\n",
    "data_dict[data_dict['Field'] == 'PCIAT-PCIAT_Total']['Value Labels'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c33ae9",
   "metadata": {},
   "source": [
    "One thing to take into account is that some of the items in the PCIAT were ignored by participants, leading to missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d274127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display PCIAT results with missing values highlighted\n",
    "train_with_sii = train[train['sii'].notna()][columns_not_in_test]\n",
    "train_with_sii[train_with_sii.isna().any(axis=1)].head().style.map(lambda x: 'background-color: #FFC0CB' if pd.isna(x) else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397214a",
   "metadata": {},
   "source": [
    "**Problem:** the SII score is calculated by adding together all of the non-NA values in the test, which means a participant can ignore every question (like the second participant) and get a score of 0 - entries like this are not valid, and will skew the results. We can assume then that the SII score can sometimes be incorrect.\n",
    "\n",
    "This can be addressed by recalculating the SII scores ourselves based on PCIAT_Total and the maximum possible score *IF* missing values were answered, ensuring that the recalculated SII meets the inteded thresholds even with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71438c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define PCIAT columns as variable\n",
    "PCIAT_cols = [f'PCIAT-PCIAT_{i+1:02d}' for i in range(20)]\n",
    "\n",
    "# recalculate SII function\n",
    "def recalculate_sii(row):\n",
    "    if pd.isna(row['PCIAT-PCIAT_Total']):\n",
    "        return np.nan\n",
    "    max_possible = row['PCIAT-PCIAT_Total'] + row[PCIAT_cols].isna().sum() * 5\n",
    "    if row['PCIAT-PCIAT_Total'] <= 30 and max_possible <= 30:\n",
    "        return 0\n",
    "    elif 31 <= row['PCIAT-PCIAT_Total'] <= 49 and max_possible <= 49:\n",
    "        return 1\n",
    "    elif 50 <= row['PCIAT-PCIAT_Total'] <= 79 and max_possible <= 79:\n",
    "        return 2\n",
    "    elif row['PCIAT-PCIAT_Total'] >= 80 and max_possible >= 80:\n",
    "        return 3\n",
    "    return np.nan\n",
    "\n",
    "train['recalc_sii'] = train.apply(recalculate_sii, axis=1)\n",
    "\n",
    "# overwriting SII with recalc_SII\n",
    "train['sii'] = train['recalc_sii']\n",
    "\n",
    "# categorizing SII scores by severity and adding \"missing\" for missing values\n",
    "train['complete_resp_total'] = train['PCIAT-PCIAT_Total'].where(train[PCIAT_cols].notna().all(axis=1), np.nan)\n",
    "sii_map = {0: '0 (None)', 1: '1 (Mild)', 2: '2 (Moderate)', 3: '3 (Severe)'}\n",
    "train['sii'] = train['sii'].map(sii_map).fillna('Missing')\n",
    "sii_order = ['Missing', '0 (None)', '1 (Mild)', '2 (Moderate)', '3 (Severe)']\n",
    "train['sii'] = pd.Categorical(train['sii'], categories=sii_order, ordered=True)\n",
    "\n",
    "# dropping recalc_SII column\n",
    "train.drop(columns='recalc_sii', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15785f0",
   "metadata": {},
   "source": [
    "### Removing Missing Values and Duplicate Rows\n",
    "Now with our SII values updated, we can remove all rows that are labeled \"missing\" in the SII column. *Why?* They're not useful to us, since we are training our models precisely to detect this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing SII\n",
    "initial_rows = len(train)\n",
    "train = train[train['sii'] != 'Missing']\n",
    "train['sii'] = train['sii'].cat.remove_unused_categories()\n",
    "removed_rows = initial_rows - len(train)\n",
    "print(f\"Removed {removed_rows} rows with 'Missing' SII values.\")\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "\n",
    "# remove duplicate rows\n",
    "duplicate_count = train.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4513e",
   "metadata": {},
   "source": [
    "Before we begin, let's quickly define a function to generate summary statistics for a given - this will make analysis easier and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define calculate stats function\n",
    "def calculate_stats(data, columns):\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    stats = []\n",
    "    for col in columns:\n",
    "        if data[col].dtype in ['object', 'category']:\n",
    "            counts = data[col].value_counts(dropna=False, sort=False)\n",
    "            percents = data[col].value_counts(normalize=True, dropna=False, sort=False) * 100\n",
    "            formatted = counts.astype(str) + ' (' + percents.round(2).astype(str) + '%)'\n",
    "            stats_col = pd.DataFrame({'count (%)': formatted})\n",
    "            stats.append(stats_col)\n",
    "        else:\n",
    "            stats_col = data[col].describe().to_frame().transpose()\n",
    "            stats_col['missing'] = data[col].isnull().sum()\n",
    "            stats_col.index.name = col\n",
    "            stats.append(stats_col)\n",
    "\n",
    "    return pd.concat(stats, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea3543",
   "metadata": {},
   "source": [
    "### Analyzing the Target Variable\n",
    "\n",
    "Let's conduct some simple analysis on the SII and PCIAT variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables for plotting\n",
    "sii_counts = train['sii'].value_counts().reset_index()\n",
    "sii_counts.columns = ['SII', 'Count']\n",
    "total = sii_counts['Count'].sum()\n",
    "sii_counts['percentage'] = (sii_counts['Count'] / total) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# custom color pallete (used throughout)\n",
    "custom_palette = ['#FFFB46', '#E2A0FF', '#B7FFD8', '#FFC1CF', '#C4F5FC']\n",
    "\n",
    "axes[0].pie(\n",
    "    sii_counts['Count'],\n",
    "    labels=sii_counts['SII'],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=custom_palette,\n",
    "    startangle=140,\n",
    "    wedgeprops={'edgecolor': 'black'}\n",
    ")\n",
    "axes[0].set_title('Distribution of Severity Impairment Index (SII)', fontsize=14)\n",
    "axes[0].axis('equal')  # Equal aspect ratio makes the pie a circle\n",
    "\n",
    "# PCIAT_Total for complete responses\n",
    "sns.histplot(train['complete_resp_total'].dropna(), bins=20, ax=axes[1])\n",
    "axes[1].set_title('Distribution of PCIAT_Total', fontsize=14)\n",
    "axes[1].set_xlabel('PCIAT_Total for Complete PCIAT Responses')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9898ec2e",
   "metadata": {},
   "source": [
    "We can notice that 60% of participants are supposedly not affected by internet use, 26% are mildly impaired, and only a little over 1% are severely impaired. PCIAT scores seem to mostly follow a normal curve, though with many participants answering 0 on all questions - with reasonable assurance, we can assume some of these responses are disingenuous. \n",
    "\n",
    "Let us now use our handy *calculate_stats* funtion to take a look at some essential demographics for participants in the study, age, sex, and SII distribution across these demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d95da",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train['Basic_Demos-Age'].isna().sum() == 0\n",
    "assert train['Basic_Demos-Sex'].isna().sum() == 0\n",
    "\n",
    "# participant age distribution\n",
    "train['Age Group'] = pd.cut(train['Basic_Demos-Age'], bins=[4, 12, 18, 22], labels=['Children (5-12)', 'Adolescents (13-18)', 'Adults (19-22)'])\n",
    "calculate_stats(train, 'Age Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# participant sex distribution\n",
    "sex_map = {0: 'Male', 1: 'Female'}\n",
    "train['Basic_Demos-Sex'] = train['Basic_Demos-Sex'].map(sex_map)\n",
    "calculate_stats(train, 'Basic_Demos-Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe060ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SII distribution by age group\n",
    "stats_age = train.groupby(['Age Group', 'sii'], observed=False).size().unstack(fill_value=0)\n",
    "stats_age_prop = stats_age.div(stats_age.sum(axis=1), axis=0) * 100\n",
    "stats_age = stats_age.astype(str) +' (' + stats_age_prop.round(1).astype(str) + '%)'\n",
    "stats_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SII distribution by sex\n",
    "stats_sex = train.groupby(['Basic_Demos-Sex', 'sii'], observed=False).size().unstack(fill_value=0)\n",
    "stats_sex_prop = stats_sex.div(stats_sex.sum(axis=1), axis=0) * 100\n",
    "stats_sex = stats_sex.astype(str) +' (' + stats_sex_prop.round(1).astype(str) + '%)'\n",
    "stats_sex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c211b7",
   "metadata": {},
   "source": [
    "The tables show that the distribution of SII scores for children and adults is skewed towards lower values, while scores for adolescents are relatively balanced across the categories. We also might notice observe that there are significantly more severe cases for males, although this is slightly misleading since the test sample is heavily skewed male - in actuality, the differences between SII distribution in males and females are not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ef1c7",
   "metadata": {},
   "source": [
    "### Internet Use and SII\n",
    "\n",
    "Now let us take a closer look at internet usage specifically to see how it relates to SII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# generate bar graph, distribution of hours of internet use\n",
    "ax1 = sns.countplot(x='PreInt_EduHx-computerinternet_hoursday', hue='PreInt_EduHx-computerinternet_hoursday', data=train, palette=custom_palette[:4], legend=False, ax=axes[0], edgecolor='black', linewidth=0.6)\n",
    "axes[0].set_title('Distribution of Hours of Internet Use')\n",
    "axes[0].set_xlabel('Hours per Day Group')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "total = len(train['PreInt_EduHx-computerinternet_hoursday'])\n",
    "for p in ax1.patches:\n",
    "    count = int(p.get_height())\n",
    "    percentage = '{:.1f}%'.format(100 * count / total)\n",
    "    ax1.annotate(f'{count} ({percentage})', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', va='baseline', fontsize=10, color='black', xytext=(0, 5), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "# generate boxplot, hours of internet use by age\n",
    "sns.boxplot(y=train['Basic_Demos-Age'], x=train['PreInt_EduHx-computerinternet_hoursday'], hue=train['PreInt_EduHx-computerinternet_hoursday'], palette=custom_palette[:4], ax=axes[1], legend=False)\n",
    "axes[1].set_title('Hours of Internet Use by Age')\n",
    "axes[1].set_ylabel('Age')\n",
    "axes[1].set_xlabel('Hours per Day Group')\n",
    "\n",
    "# generate boxplot, internet hours by age group\n",
    "sns.boxplot(y='PreInt_EduHx-computerinternet_hoursday', x='Age Group', hue='Age Group',data=train, palette=custom_palette[:3], ax=axes[2], legend=False)\n",
    "axes[2].set_title('Internet Hours by Age Group')\n",
    "axes[2].set_ylabel('Hours per Day (Numeric)')\n",
    "axes[2].set_xlabel('Age Group');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde29ed",
   "metadata": {},
   "source": [
    "Similar to the earlier SII data, we can see that higher daily internet usage is associated with older age. This hints towards a correlation between internet use and SII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20af42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "gs = fig.add_gridspec(2, 2, height_ratios=[1, 1.5])\n",
    "\n",
    "# generate boxplot, SII by hours of internet use\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.boxplot(x='sii', y='PreInt_EduHx-computerinternet_hoursday', hue='sii', data=train, ax=ax1, palette=custom_palette[:4], legend=False)\n",
    "ax1.set_title('SII by Hours of Internet Use')\n",
    "ax1.set_ylabel('Hours per Day')\n",
    "ax1.set_xlabel('SII')\n",
    "\n",
    "# generate boxplot, PCIAT_Total by hours of internet use\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "sns.boxplot(x='PreInt_EduHx-computerinternet_hoursday', y='complete_resp_total', hue='PreInt_EduHx-computerinternet_hoursday', data=train, palette=custom_palette[:4], ax=ax2, legend=False)\n",
    "ax2.set_title('PCIAT_Total by Hours of Internet Use')\n",
    "ax2.set_ylabel('PCIAT_Total for Complete PCIAT Responses')\n",
    "ax2.set_xlabel('Hours per Day Group');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccc6e9",
   "metadata": {},
   "source": [
    "Simply tracing your finger along across these plots, a slight upward trend is visible between SII/ PCIAT and hours of internet use, showing their correlation. It might be notable that there are plenty of participants across all age groups that report less than an hour of internet use and still having a high SII score, suggesting that only an hour of internet use can be enough to negatively impact mental health. \n",
    "\n",
    "## Exploring Features Related to Physical Activity\n",
    "\n",
    "A significant portion of the feature data gives information on participants' physical measures, like height, weight, BMI, heart rate, etc. The relationship between internet use and SII is intuitive, but physical characteristics? Results in this analysis might yield some interesting and unexpected results.\n",
    "\n",
    "There just so happen to be 7 primary physical features in the train set - this is the perfect number of features for building a **correlation matrix**, where each feature is given a score from -1 to 1 based on its correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert SII values to float\n",
    "train['sii'] = train['sii'].str[0].astype(float)\n",
    "\n",
    "# identify physical measures\n",
    "cols = ['Physical-BMI', 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP']\n",
    "data_subset = train[cols + ['sii']]\n",
    "\n",
    "# generate correlation matrix\n",
    "corr_matrix = data_subset.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362144b4",
   "metadata": {},
   "source": [
    "This matrix gives a pretty comprehensive overview of how physical features are interrelated - in the top right corener, we see BMI, height, weight and waist circumference all highly correlated with one another, as we migtht expect. But what we're really interested in is how these features are related with SII.\n",
    "\n",
    "Reading the bottom row, we see that height, weight and waist circumference are positively correlated with SII, while dialostic blood pressure, systolic blood pressure and heart rate are weakly correlated. This would suggest that cardiovascular health is not a determining factor for SII score, although it should be noted that the positive correlation could actually our previously observed age-correlation in disguise, since physical metrics increase with age.\n",
    "\n",
    "Let us take a look at height and weight by age. We can assume BMI will follow from these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# physical height by age\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(x='Basic_Demos-Age', y='Physical-Height', data=train)\n",
    "plt.title('Physical-Height by Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Height (cm)')\n",
    "\n",
    "# physical weight by age\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(x='Basic_Demos-Age', y='Physical-Weight', data=train)\n",
    "plt.title('Physical-Weight by Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Weight (kg)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af60c7a",
   "metadata": {},
   "source": [
    "Unsurprisingly, physical metrics are positively correlated with age. We can also see a number of outliers in the weight measurements, which might suggest a slight increase in correlation between weight and SII. Comparing physical metrics with SII directly, we get the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05149478",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# boxplot: SII vs BMI\n",
    "sns.boxplot(x='sii', y='Physical-BMI', data=train, hue='sii', ax=axes[0], palette=custom_palette[:4], legend=False)\n",
    "axes[0].set_title('SII vs BMI')\n",
    "axes[0].set_xlabel('SII Category')\n",
    "axes[0].set_ylabel('BMI')\n",
    "\n",
    "# boxplot: SII vs Height\n",
    "sns.boxplot(x='sii', y='Physical-Height', data=train, hue='sii', ax=axes[1], palette=custom_palette[:4], legend=False)\n",
    "axes[1].set_title('SII vs Height')\n",
    "axes[1].set_xlabel('SII Category')\n",
    "axes[1].set_ylabel('Height (cm)')\n",
    "\n",
    "# boxplot: SII vs Weight\n",
    "sns.boxplot(x='sii', y='Physical-Weight', data=train, hue='sii', ax=axes[2], palette=custom_palette[:4], legend=False)\n",
    "axes[2].set_title('SII vs Weight')\n",
    "axes[2].set_xlabel('SII Category')\n",
    "axes[2].set_ylabel('Weight (kg)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b66ca",
   "metadata": {},
   "source": [
    "Here we can see something kind of interesting - there's a pretty sharp jump in BMI and weight statistics when SII is 3. This jump is less pronounced with height, seeming to suggest that BMI increases through its relation to weight specifically. This gives us a clue that **weight** might play a significant factor in higher SII scores, which makes intuitive sense.\n",
    "\n",
    "Let us now take a look at the top 20 features based on correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate non-PCIAT features\n",
    "train_subset = train.drop(train.columns[55:76].tolist() + ['complete_resp_total'], axis=1)\n",
    "numerical_features = train_subset.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# calculate Pearson correlations with 'sii'\n",
    "target_correlations = []\n",
    "for feature in numerical_features:\n",
    "    if feature != 'sii':\n",
    "        corr = train_subset[[feature, 'sii']].corr().iloc[0, 1]\n",
    "        if not np.isnan(corr):\n",
    "            target_correlations.append((feature, abs(corr)))\n",
    "\n",
    "# sort and take top 20\n",
    "top_corr = sorted(target_correlations, key=lambda x: x[1], reverse=True)[:20]\n",
    "features, corrs = zip(*top_corr)\n",
    "\n",
    "# plot features\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(features)), corrs, color='#E2A0FF', edgecolor='black')\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.xlabel('Absolute Correlation with SII')\n",
    "plt.title('Top 20 Features by Correlation with SII')\n",
    "plt.gca().invert_yaxis()  # Optional: highest at top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a444b",
   "metadata": {},
   "source": [
    "As we found earlier, features like height, age, internet use and weight top the list. We also see positive correlations with other features we did not look as closely into - these other features will play a role during the feature engineering and selection process.\n",
    "\n",
    "Last thing we'll do for this section is remove all features that are directly related to SII, like PCIAT scores, and arbitrary identifiers like \"id\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456639ee",
   "metadata": {},
   "source": [
    "\n",
    "# Model Training üèã\n",
    "\n",
    "With an strong understanding of the data at hand, we are ready to tackle model training! üéâ\n",
    "\n",
    "Since we're attacking this problem with an ensemble appraoch, this means creating separate pipelines for pre-processing, feature engineering, training and validation. Will this add an extra layer of complexity? Yes. But, it will get us closer to an accurate solution, which is not biased to any one model, increasing our chances of an accurate prediction. We're going to train a **Ridge model**, which is a linear regression model intended for interpretability, and a **LightGBM model**, a slightly more advanced model that can handle non-linear relationships and complex interactions between features - together, these models should provide a balanced foundation that we can tune and optimize. Here is a basic workflow for this section:\n",
    "\n",
    "*‚Ä£ General pre-processing for all data*\n",
    "\n",
    "*‚Ä£ General feature engineering for all data based on EDA and correlation*\n",
    "\n",
    "*‚Ä£ Split data into \"train\" and \"validation\" set*\n",
    "\n",
    "*‚Ä£ Ridge model training: feature selection, train and validate on validation set with various correlation thresholds, then select best model based on RMSE (Root Mean Squared Error)*\n",
    "\n",
    "*‚Ä£ LightGBM model training: rank features based on importance, define model parameters,train and validate on validation set*\n",
    "\n",
    "*‚Ä£ Conduct analysis on the results of our training and prepare for model tuning*\n",
    "\n",
    "First, let's remove all features that are directly related to SII, like PCIAT scores, and arbitrary identifiers like \"id\". This will prevent data leakage and possible overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all PCIAT columns\n",
    "pciat_cols = [col for col in train.columns if 'PCIAT' in col]\n",
    "train = train.drop(columns=pciat_cols)\n",
    "\n",
    "# remove \"complete_resp\" and \"age group\" columns\n",
    "train = train.drop(\"complete_resp_total\", axis=1)\n",
    "train = train.drop(\"Age Group\", axis=1) \n",
    "\n",
    "# remove id column\n",
    "train = train.drop(\"id\", axis=1)\n",
    "\n",
    "# train.to_csv(\"/Users/tomragus/Library/CloudStorage/OneDrive-UCSanDiego/CMI-PIU-Model/data/train_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7f6c1",
   "metadata": {},
   "source": [
    "Next, let's define some variables, separate the target variable from the dataset, and re-calculate Pearson correlations - these correlations will come in handy for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba7cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target variable from features\n",
    "X = train.drop('sii', axis=1)\n",
    "y = train['sii']\n",
    "\n",
    "# sets for categorical and numerical features\n",
    "categorical_cols = []\n",
    "numerical_cols = []\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        categorical_cols.append(col)\n",
    "    else:\n",
    "        numerical_cols.append(col)\n",
    "\n",
    "# re-calculate Pearson correlations with target for numerical features\n",
    "def calculate_correlations(X, y, numerical_cols):\n",
    "    correlations = {}\n",
    "    for col in numerical_cols:\n",
    "        mask = ~(X[col].isnull() | y.isnull())\n",
    "        if mask.sum() > 1:\n",
    "            corr, p_value = pearsonr(X[col][mask], y[mask])\n",
    "            correlations[col] = {'correlation': corr, 'p_value': p_value}\n",
    "    return correlations\n",
    "\n",
    "correlations = calculate_correlations(X, y, numerical_cols)\n",
    "\n",
    "# sort by absolute correlation\n",
    "sorted_correlations = sorted(correlations.items(), \n",
    "                           key=lambda x: abs(x[1]['correlation']), \n",
    "                           reverse=True)\n",
    "\n",
    "print(\"\\nTop 20 correlations with SII:\")\n",
    "for i, (col, stats) in enumerate(sorted_correlations[:20]):\n",
    "    print(f\"{i+1:2d}. {col:<40} | Corr: {stats['correlation']:6.3f} | p-value: {stats['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a1eeb",
   "metadata": {},
   "source": [
    "Here we will define some new features, some of my own creation, and some based on our calculated correlations. These new features, on top of the existing features, will give the models a lot to work with.\n",
    "\n",
    "‚Ä£ Difference between BMI and BIA\n",
    "\n",
    "‚Ä£ Total exercise duration\n",
    "\n",
    "‚Ä£ Ratio of Fat to FFM\n",
    "\n",
    "‚Ä£ Z-score normalation of cardiovascular health statistics (heart rate, systolic and diastolic blood pressure)\n",
    "\n",
    "‚Ä£ Age squared\n",
    "\n",
    "‚Ä£ Age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining feature engineering functions\n",
    "def create_engineered_features(df, correlations=None):\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # BMI-related features\n",
    "    if 'Physical-BMI' in df_eng.columns and 'BIA-BIA_BMI' in df_eng.columns:\n",
    "        df_eng['BMI_difference'] = df_eng['Physical-BMI'] - df_eng['BIA-BIA_BMI']\n",
    "    \n",
    "    # fitness ratios\n",
    "    if 'Fitness_Endurance-Time_Mins' in df_eng.columns and 'Fitness_Endurance-Time_Sec' in df_eng.columns:\n",
    "        df_eng['Total_Fitness_Time'] = df_eng['Fitness_Endurance-Time_Mins'] * 60 + df_eng['Fitness_Endurance-Time_Sec']\n",
    "    \n",
    "    # body composition ratios\n",
    "    if 'BIA-BIA_Fat' in df_eng.columns and 'BIA-BIA_FFM' in df_eng.columns:\n",
    "        df_eng['Fat_to_FFM_ratio'] = df_eng['BIA-BIA_Fat'] / (df_eng['BIA-BIA_FFM'] + 1e-8)\n",
    "    \n",
    "    # physical health composite\n",
    "    if all(col in df_eng.columns for col in ['Physical-HeartRate', 'Physical-Systolic_BP', 'Physical-Diastolic_BP']):\n",
    "        health_cols = ['Physical-HeartRate', 'Physical-Systolic_BP', 'Physical-Diastolic_BP']\n",
    "        for col in health_cols:\n",
    "            if df_eng[col].notna().sum() > 0:\n",
    "                mean_val = df_eng[col].mean()\n",
    "                std_val = df_eng[col].std()\n",
    "                df_eng[f'{col}_normalized'] = (df_eng[col] - mean_val) / (std_val + 1e-8)\n",
    "    \n",
    "    # age-related\n",
    "    if 'Basic_Demos-Age' in df_eng.columns:\n",
    "        df_eng['Age_squared'] = df_eng['Basic_Demos-Age'] ** 2\n",
    "        df_eng['Age_group'] = pd.cut(df_eng['Basic_Demos-Age'], \n",
    "                                   bins=[0, 8, 12, 16, 22], \n",
    "                                   labels=['child', 'preteen', 'teen', 'young_adult'])\n",
    "    \n",
    "    # create correlation-based features\n",
    "    if correlations is not None:\n",
    "        top_corr_features = [col for col, stats in sorted(correlations.items(), key=lambda x: abs(x[1]['correlation']), reverse=True)[:10] if col in df_eng.columns]\n",
    "        \n",
    "        for i, col1 in enumerate(top_corr_features[:5]):\n",
    "            for col2 in top_corr_features[i+1:5]:\n",
    "                if col1 in df_eng.columns and col2 in df_eng.columns:\n",
    "                    if df_eng[col1].dtype in ['int64', 'float64'] and df_eng[col2].dtype in ['int64', 'float64']:\n",
    "                        df_eng[f'{col1}_x_{col2}'] = df_eng[col1] * df_eng[col2]\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "# apply feature engineering\n",
    "X_engineered = create_engineered_features(X, correlations)\n",
    "\n",
    "print(\"New features created:\")\n",
    "new_features = set(X_engineered.columns) - set(X.columns)\n",
    "for feat in new_features:\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d3251a",
   "metadata": {},
   "source": [
    "Keeping track of and separating data by datatype will allow our models to digest the data, so here we will organize data by type - we will also recalculate Pearson correlation, now that we have additional features to account for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff16103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create categorical and numerical sets with engineered features\n",
    "categorical_cols_eng = []\n",
    "numerical_cols_eng = []\n",
    "\n",
    "for col in X_engineered.columns:\n",
    "    if is_object_dtype(X_engineered[col]) or isinstance(X_engineered[col].dtype, CategoricalDtype):\n",
    "        categorical_cols_eng.append(col)\n",
    "    elif is_numeric_dtype(X_engineered[col]):\n",
    "        numerical_cols_eng.append(col)\n",
    "\n",
    "print(f\"\\nFinal categorical columns: {len(categorical_cols_eng)}\")\n",
    "print(f\"Final numerical columns: {len(numerical_cols_eng)}\")\n",
    "\n",
    "# recalculate correlations with engineered features\n",
    "correlations_eng = calculate_correlations(X_engineered, y, numerical_cols_eng)\n",
    "\n",
    "# sort by absolute correlation\n",
    "sorted_correlations_eng = sorted(correlations_eng.items(), key=lambda x: abs(x[1]['correlation']), reverse=True)\n",
    "\n",
    "print(\"\\nTop 15 correlations with SII (after feature engineering):\")\n",
    "for i, (col, stats) in enumerate(sorted_correlations_eng[:15]):\n",
    "    print(f\"{i+1:2d}. {col:<40} | Corr: {stats['correlation']:6.3f} | p-value: {stats['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c1e9f",
   "metadata": {},
   "source": [
    "Notice, every feature with \"_x_\" is one of the features created based on correlations with eachother. And look at that! A significant portion of our features with the highest correlation with SII are these new created features. You can also features like \"age squared\" making their way into the list. Now, we have a wide selection of **highly correlated** features to work with.\n",
    "\n",
    "We're almost ready to train the models - but before we do, let us split our data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split engineered data into train and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30195960",
   "metadata": {},
   "source": [
    "Time for training! üèãÔ∏è\n",
    "\n",
    "For the Ridge model, we'll want to do a few things. **First,** we'll want to select the features to use for the model. We'll do this by using the correlation filtering method. **Second,** we will account for missing values in the dataset, filling empty numerical data with the median, and empty categorical data with the mode. **Third,** we will scale the features to have zero mean and unit variance, and encode the categorical variables using one-hot encoding. We'll train the models with a few different **correlation thresholds** (meaning, only features meeting a certain correlation threshold will be included in the training), as well as one with no threshold, and for each evaluate the model on our validation set, selecting the model with the **lowest validation RMSE.** \n",
    "\n",
    "The thresholds I will use are 0.01, 0.03, 0.05, and 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24254c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIDGE REGRESSION PREPROCESSING PIPELINE\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RIDGE REGRESSION PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# define function for feature selection based on threshold\n",
    "def select_features_by_correlation(X, y, numerical_cols, threshold=0.05):\n",
    "    correlations = calculate_correlations(X, y, numerical_cols)\n",
    "    \n",
    "    selected_features = []\n",
    "    for col, stats in correlations.items():\n",
    "        if abs(stats['correlation']) >= threshold and stats['p_value'] < 0.05:\n",
    "            selected_features.append(col)\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "# define ridge model preprocessing pipeline\n",
    "def preprocess_for_ridge(X_train, X_val, categorical_cols, numerical_cols, correlation_threshold=0.05, use_correlation_filtering=True):\n",
    "    \n",
    "    # create copies of data\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_val_processed = X_val.copy()\n",
    "    \n",
    "    # handle missing values (fill numerical with median, categorical with mode)\n",
    "    for col in numerical_cols:\n",
    "        if col in X_train_processed.columns:\n",
    "            median_val = X_train_processed[col].median()\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(median_val)\n",
    "            X_val_processed[col] = X_val_processed[col].fillna(median_val)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in X_train_processed.columns:\n",
    "            mode_val = X_train_processed[col].mode()[0] if not X_train_processed[col].mode().empty else 'Unknown'\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(mode_val)\n",
    "            X_val_processed[col] = X_val_processed[col].fillna(mode_val)\n",
    "    \n",
    "    # feature selection based on correlation\n",
    "    selected_numerical_features = numerical_cols\n",
    "    if use_correlation_filtering:\n",
    "        selected_numerical_features = select_features_by_correlation(\n",
    "            X_train_processed, y_train, numerical_cols, correlation_threshold\n",
    "        )\n",
    "        \n",
    "        print(f\"Selected {len(selected_numerical_features)} numerical features out of {len(numerical_cols)} based on correlation > {correlation_threshold}\")\n",
    "        print(\"Selected features:\", selected_numerical_features[:10], \"...\" if len(selected_numerical_features) > 10 else \"\")\n",
    "        \n",
    "        # keep only selected numerical features\n",
    "        features_to_keep = selected_numerical_features + categorical_cols\n",
    "        X_train_processed = X_train_processed[features_to_keep]\n",
    "        X_val_processed = X_val_processed[features_to_keep]\n",
    "    \n",
    "    # one-hot encoding for categorical variables\n",
    "    X_train_encoded = pd.get_dummies(X_train_processed, columns=categorical_cols, drop_first=True)\n",
    "    X_val_encoded = pd.get_dummies(X_val_processed, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # validate continuity between sets\n",
    "    missing_cols_val = set(X_train_encoded.columns) - set(X_val_encoded.columns)\n",
    "    missing_cols_train = set(X_val_encoded.columns) - set(X_train_encoded.columns)\n",
    "    \n",
    "    for col in missing_cols_val:\n",
    "        X_val_encoded[col] = 0\n",
    "    for col in missing_cols_train:\n",
    "        X_train_encoded[col] = 0\n",
    "    \n",
    "    X_val_encoded = X_val_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "    \n",
    "    # scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "    X_val_scaled = scaler.transform(X_val_encoded)\n",
    "    \n",
    "    encoded_cols = list(X_train_encoded.columns)\n",
    "\n",
    "    return X_train_scaled, X_val_scaled, scaler, encoded_cols, selected_numerical_features\n",
    "\n",
    "# define different correlation thresholds\n",
    "correlation_thresholds = [0.01, 0.03, 0.05, 0.1]\n",
    "ridge_results = {}\n",
    "\n",
    "print(\"\\nTesting different correlation thresholds for Ridge:\")\n",
    "for threshold in correlation_thresholds:\n",
    "    print(f\"\\n--- Testing threshold: {threshold} ---\")\n",
    "    \n",
    "    # preprocessing\n",
    "    X_train_ridge, X_val_ridge, scaler, encoded_cols, selected_features = preprocess_for_ridge(\n",
    "        X_train, X_val, categorical_cols_eng, numerical_cols_eng, \n",
    "        correlation_threshold=threshold, use_correlation_filtering=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Ridge - Train shape after preprocessing: {X_train_ridge.shape}\")\n",
    "    \n",
    "    # train Ridge Regression\n",
    "    ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "    ridge_model.fit(X_train_ridge, y_train)\n",
    "    \n",
    "    # predictions and evaluation\n",
    "    y_train_pred_ridge = ridge_model.predict(X_train_ridge)\n",
    "    y_val_pred_ridge = ridge_model.predict(X_val_ridge)\n",
    "    \n",
    "    ridge_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))\n",
    "    ridge_val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_ridge))\n",
    "    \n",
    "    ridge_results[threshold] = {\n",
    "        'train_rmse': ridge_train_rmse,\n",
    "        'val_rmse': ridge_val_rmse,\n",
    "        'num_features': X_train_ridge.shape[1],\n",
    "        'selected_features': selected_features\n",
    "    }\n",
    "    \n",
    "    print(f\"Train RMSE: {ridge_train_rmse:.4f}\")\n",
    "    print(f\"Validation RMSE: {ridge_val_rmse:.4f}\")\n",
    "\n",
    "# testing without correlation fitting\n",
    "print(f\"\\n--- Testing without correlation filtering ---\")\n",
    "X_train_ridge_no_filter, X_val_ridge_no_filter, ridge_scaler_no_filter, ridge_columns_no_filter, _ = preprocess_for_ridge(\n",
    "    X_train, X_val, categorical_cols_eng, numerical_cols_eng, \n",
    "    use_correlation_filtering=False\n",
    ")\n",
    "\n",
    "print(f\"Ridge - Train shape after preprocessing: {X_train_ridge_no_filter.shape}\")\n",
    "\n",
    "ridge_model_no_filter = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model_no_filter.fit(X_train_ridge_no_filter, y_train)\n",
    "\n",
    "y_train_pred_ridge_no_filter = ridge_model_no_filter.predict(X_train_ridge_no_filter)\n",
    "y_val_pred_ridge_no_filter = ridge_model_no_filter.predict(X_val_ridge_no_filter)\n",
    "\n",
    "ridge_train_rmse_no_filter = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge_no_filter))\n",
    "ridge_val_rmse_no_filter = np.sqrt(mean_squared_error(y_val, y_val_pred_ridge_no_filter))\n",
    "\n",
    "ridge_results['no_filter'] = {\n",
    "    'train_rmse': ridge_train_rmse_no_filter,\n",
    "    'val_rmse': ridge_val_rmse_no_filter,\n",
    "    'num_features': X_train_ridge_no_filter.shape[1],\n",
    "    'selected_features': None\n",
    "}\n",
    "\n",
    "print(f\"Train RMSE: {ridge_train_rmse_no_filter:.4f}\")\n",
    "print(f\"Validation RMSE: {ridge_val_rmse_no_filter:.4f}\")\n",
    "\n",
    "# selecting best Ridge configuration for final model\n",
    "best_ridge_config = min(ridge_results.items(), key=lambda x: x[1]['val_rmse'])\n",
    "print(f\"\\nBest Ridge configuration: {best_ridge_config[0]} (Validation RMSE: {best_ridge_config[1]['val_rmse']:.4f})\")\n",
    "\n",
    "if best_ridge_config[0] == 'no_filter':\n",
    "    X_train_ridge_final, X_val_ridge_final = X_train_ridge_no_filter, X_val_ridge_no_filter\n",
    "    ridge_model_final = ridge_model_no_filter\n",
    "    final_ridge_train_rmse, final_ridge_val_rmse = ridge_train_rmse_no_filter, ridge_val_rmse_no_filter\n",
    "else:\n",
    "    X_train_ridge_final, X_val_ridge_final, _, _, _ = preprocess_for_ridge(\n",
    "        X_train, X_val, categorical_cols_eng, numerical_cols_eng, \n",
    "        correlation_threshold=best_ridge_config[0], use_correlation_filtering=True\n",
    "    )\n",
    "    ridge_model_final = Ridge(alpha=1.0, random_state=42)\n",
    "    ridge_model_final.fit(X_train_ridge_final, y_train)\n",
    "    \n",
    "    y_train_pred_final = ridge_model_final.predict(X_train_ridge_final)\n",
    "    y_val_pred_final = ridge_model_final.predict(X_val_ridge_final)\n",
    "    \n",
    "    final_ridge_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_final))\n",
    "    final_ridge_val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03af0da",
   "metadata": {},
   "source": [
    "BOOM! üí£\n",
    "\n",
    "We have our 5 models, one for each threshold and one with no threshold. Observing the results, we can see training with a testing threshold of 0.1 gives us the lowest validation RMSE of 0.6768. *How do we interpret this result?* This means that on average, when predicting SII (recall SII ranges from 0 to 3), our model is off by 0.6768 points. Not bad - there is room for improvement, but the model is frequently correct or close to correct. In this result we also have a train RMSE of 0.663. This slight difference between RMSE scores suggest slight overfitting, meaning the model performs better on the training data than it does on the validation data. \n",
    "\n",
    "Now time to tackle the LightGBM model - just like with Ridge, we'll fill missing values and split into train and validation sets. One thing about LightGBM is that it automatically assigns an \"immportance\" value to each feature, removing the need to utilize correlation filtering. Many of the things we did manually for Ridge, LightGBM does internally - all we do is tell LigthGBM what parameters to use, and it does the rest (rewrite this part EW!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGHTGBM PREPROCESSING PIPELINE\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LIGHTGBM PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def preprocess_for_lightgbm(X_train, X_val, categorical_cols, numerical_cols):\n",
    "    \n",
    "    # create copies of data\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_val_processed = X_val.copy()\n",
    "    \n",
    "    # handle missing values\n",
    "    for col in numerical_cols:\n",
    "        if col in X_train_processed.columns:\n",
    "            median_val = X_train_processed[col].median()\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(median_val)\n",
    "            X_val_processed[col] = X_val_processed[col].fillna(median_val)\n",
    "    \n",
    "    # label encoding for categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if col in X_train_processed.columns:\n",
    "            le = LabelEncoder()\n",
    "            \n",
    "            # fill missing values first\n",
    "            mode_val = X_train_processed[col].mode()[0] if not X_train_processed[col].mode().empty else 'Unknown'\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(mode_val)\n",
    "            X_val_processed[col] = X_val_processed[col].fillna(mode_val)\n",
    "\n",
    "            # fit on train data\n",
    "            X_train_processed[col] = le.fit_transform(X_train_processed[col]).astype(int)\n",
    "\n",
    "            # handle unseen categories in validation\n",
    "            train_categories = set(le.classes_)\n",
    "            X_val_processed[col] = X_val_processed[col].map(\n",
    "                lambda x: le.transform([x])[0] if x in train_categories else 0\n",
    "            ).astype(int)\n",
    "\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return X_train_processed, X_val_processed, label_encoders\n",
    "\n",
    "# preprocess for LightGBM\n",
    "X_train_lgb, X_val_lgb, lgb_encoders = preprocess_for_lightgbm(\n",
    "    X_train, X_val, categorical_cols_eng, numerical_cols_eng\n",
    ")\n",
    "\n",
    "categorical_feature_names = [col for col in categorical_cols_eng if col in X_train_lgb.columns]\n",
    "\n",
    "print(f\"LightGBM - Train shape after preprocessing: {X_train_lgb.shape}\")\n",
    "print(f\"LightGBM - Validation shape after preprocessing: {X_val_lgb.shape}\")\n",
    "\n",
    "# define LightGBM parameters\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'subsample': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'random_state': 42,\n",
    "    'force_col_wise': True\n",
    "}\n",
    "\n",
    "# create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train_lgb, label=y_train, categorical_feature=categorical_feature_names)\n",
    "val_data = lgb.Dataset(X_val_lgb, label=y_val, reference=train_data, categorical_feature=categorical_feature_names)\n",
    "\n",
    "# train LightGBM model\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "# predictions and evaluation\n",
    "y_train_pred_lgb = lgb_model.predict(X_train_lgb, num_iteration=lgb_model.best_iteration)\n",
    "y_val_pred_lgb = lgb_model.predict(X_val_lgb, num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "lgb_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_lgb))\n",
    "lgb_val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_lgb))\n",
    "\n",
    "print(f\"\\nLightGBM Results:\")\n",
    "print(f\"Train RMSE: {lgb_train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {lgb_val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040180d1",
   "metadata": {},
   "source": [
    "With LightGBM, we get a slightly lower validation RMSE of 0.6693. Again, a decent score! This time however, we notice a more significant difference between the train RMSE of 0.5296 and the validation RMSE, which is evidence of overfitting. \n",
    "\n",
    "Now with both of our models trained, let's compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e9abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate final model comparison\n",
    "print(f\"\\nFinal Ridge Regression (Best Config: {best_ridge_config[0]}):\")\n",
    "print(f\"  Train RMSE: {final_ridge_train_rmse:.4f}\")\n",
    "print(f\"  Validation RMSE: {final_ridge_val_rmse:.4f}\")\n",
    "print(f\"  Overfitting: {final_ridge_val_rmse - final_ridge_train_rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nLightGBM (All Features):\")\n",
    "print(f\"  Train RMSE: {lgb_train_rmse:.4f}\")\n",
    "print(f\"  Validation RMSE: {lgb_val_rmse:.4f}\")\n",
    "print(f\"  Overfitting: {lgb_val_rmse - lgb_train_rmse:.4f}\")\n",
    "\n",
    "# generate features selected for Ridge\n",
    "if best_ridge_config[0] != 'no_filter':\n",
    "    print(f\"\\nFeatures selected for Ridge (correlation > {best_ridge_config[0]}):\")\n",
    "    selected_features = best_ridge_config[1]['selected_features']\n",
    "    for i, feat in enumerate(selected_features[:15]):  # Show first 15\n",
    "        corr_val = correlations_eng.get(feat, {}).get('correlation', 0)\n",
    "        print(f\"{i+1:2d}. {feat:<40} | Corr: {corr_val:6.3f}\")\n",
    "    if len(selected_features) > 15:\n",
    "        print(f\"... and {len(selected_features) - 15} more features\")\n",
    "\n",
    "# generate feature importance for LightGBM\n",
    "print(f\"\\nTop 10 Most Important Features (LightGBM):\")\n",
    "feature_importance = lgb_model.feature_importance(importance_type='gain')\n",
    "feature_names = X_train_lgb.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
    "    print(f\"{i+1:2d}. {row['feature']:<40} | Importance: {row['importance']:8.0f}\")\n",
    "\n",
    "# generate overfitting analysis\n",
    "print(\"\\nOverfitting Analysis:\")\n",
    "ridge_overfit = final_ridge_val_rmse - final_ridge_train_rmse\n",
    "lgb_overfit = lgb_val_rmse - lgb_train_rmse\n",
    "print(f\"   - Ridge overfitting: {ridge_overfit:.4f}\")\n",
    "print(f\"   - LightGBM overfitting: {lgb_overfit:.4f}\")\n",
    "if ridge_overfit < lgb_overfit:\n",
    "    print(\"   ‚Üí Ridge is more robust (less overfitting)\")\n",
    "else:\n",
    "    print(\"   ‚Üí LightGBM is more robust (less overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc9d45",
   "metadata": {},
   "source": [
    "Here are some key takeaways:\n",
    "\n",
    "*‚Ä£ Both models preformed fairly well on the validation set, with the Ridge model performing slightly better.*\n",
    "\n",
    "*‚Ä£ Looking at the features ranked by correlation for the Ridge model and comparing with the features ranked by importance by the LightGBM model, we see many of the same features high on the list, like Age, Physical Weight, etc. This is good and shows a level of consistency between the models.*\n",
    "\n",
    "*‚Ä£ Both models show signs of overfitting, with the effect being more pronounced in the LightGBM model. This will be a key point of focus during the tuning phase.*\n",
    "\n",
    "blah blah blah (leading into tuning)\n",
    "\n",
    "## Tuning Our Models üìª\n",
    "\n",
    "blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63080e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Ridge alpha tuning after selecting best preprocessing config\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge = Ridge(random_state=42)\n",
    "param_grid = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(X_train_ridge_final, y_train)\n",
    "\n",
    "print(f\"Best alpha: {grid_search.best_params_['alpha']}\")\n",
    "\n",
    "# Retrain with best alpha from CV\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "\n",
    "ridge_model_final = Ridge(alpha=best_alpha, random_state=42)\n",
    "ridge_model_final.fit(X_train_ridge_final, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_train_pred = ridge_model_final.predict(X_train_ridge_final)\n",
    "y_val_pred = ridge_model_final.predict(X_val_ridge_final)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "print(f\"\\nFinal Ridge Model with alpha={best_alpha}:\")\n",
    "print(f\"  Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  Validation RMSE: {val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136a531",
   "metadata": {},
   "source": [
    "blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'num_leaves': [15, 31],\n",
    "    'max_depth': [-1, 5],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'min_child_samples': [20, 50]\n",
    "}\n",
    "\n",
    "lgb_params['verbose'] = -1\n",
    "\n",
    "# Define base model\n",
    "lgb_model = LGBMRegressor(\n",
    "    objective='regression',\n",
    "    random_state=42,\n",
    "    n_estimators=1000  # early stopping will prevent overtraining\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',  # RMSE\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on your preprocessed LightGBM training data\n",
    "grid_search.fit(X_train_lgb, y_train)\n",
    "\n",
    "print(f\"Best Parameters:\\n{grid_search.best_params_}\")\n",
    "print(f\"Best CV RMSE: {-grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_lgb_params = grid_search.best_params_\n",
    "\n",
    "# Final model with tuned params\n",
    "final_lgb_model = LGBMRegressor(\n",
    "    **best_lgb_params,\n",
    "    objective='regression',\n",
    "    random_state=42,\n",
    "    n_estimators=1000\n",
    ")\n",
    "\n",
    "# Fit on full training set with early stopping on validation\n",
    "final_lgb_model.fit(\n",
    "    X_train_lgb, y_train,\n",
    "    eval_set=[(X_val_lgb, y_val)],\n",
    "    eval_metric='rmse',\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=50),\n",
    "        log_evaluation(0)  # 0 disables log output\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_train_pred = final_lgb_model.predict(X_train_lgb)\n",
    "y_val_pred = final_lgb_model.predict(X_val_lgb)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "print(f\"\\nTuned LightGBM Results:\")\n",
    "print(f\"  Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  Validation RMSE: {val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41208251",
   "metadata": {},
   "source": [
    "## FINAL MODEL RESULTS:\n",
    "\n",
    "‚Ä£ Ridge Regression: 0.6710\n",
    "\n",
    "‚Ä£ LightGBM: 0.6657\n",
    "\n",
    "## Training the Test Set: Ridge Model\n",
    "\n",
    "blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sex binary to string\n",
    "test['Basic_Demos-Sex'] = test['Basic_Demos-Sex'].map({0: 'Male', 1: 'Female'})\n",
    "\n",
    "# separate id from test set\n",
    "test_ids = test[\"id\"].copy()\n",
    "test = test.drop(\"id\", axis=1)\n",
    "\n",
    "# update test set with engineered features for Ridge model\n",
    "X_test_engineered = create_engineered_features(test, correlations)\n",
    "\n",
    "def clip_predictions(predictions):\n",
    "    # Round predictions to nearest integer\n",
    "    rounded = np.round(predictions)\n",
    "    # Clip values to be between 0 and 3\n",
    "    clipped = np.clip(rounded, 0, 3)\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9ffa5",
   "metadata": {},
   "source": [
    "blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_test_for_ridge(X_test, categorical_cols, numerical_cols, \n",
    "                             selected_numerical_features, scaler, encoded_column_names):\n",
    "    X_test_processed = X_test.copy()\n",
    "    \n",
    "    # handle missing values (fill numerical with median, categorical with mode)\n",
    "    for col in numerical_cols:\n",
    "        if col in X_test_processed.columns:\n",
    "            median_val = X_test_processed[col].median()\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(median_val)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in X_test_processed.columns:\n",
    "            mode_val = X_test_processed[col].mode()[0] if not X_test_processed[col].mode().empty else 'Unknown'\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(mode_val)\n",
    "\n",
    "    # Keep only the features selected during training\n",
    "    features_to_keep = selected_numerical_features + categorical_cols\n",
    "    X_test_processed = X_test_processed[features_to_keep]\n",
    "\n",
    "    # One-hot encode using same method\n",
    "    X_test_encoded = pd.get_dummies(X_test_processed, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Add any missing columns (present in training but not in test)\n",
    "    for col in encoded_column_names:\n",
    "        if col not in X_test_encoded.columns:\n",
    "            X_test_encoded[col] = 0\n",
    "\n",
    "    # Reorder columns to match training\n",
    "    X_test_encoded = X_test_encoded[encoded_column_names]\n",
    "\n",
    "    # Scale using training scaler\n",
    "    X_test_scaled = scaler.transform(X_test_encoded)\n",
    "    \n",
    "    return X_test_scaled\n",
    "\n",
    "\n",
    "X_test_scaled = preprocess_test_for_ridge(\n",
    "    X_test_engineered,\n",
    "    categorical_cols = categorical_cols_eng,\n",
    "    numerical_cols = numerical_cols_eng,\n",
    "    selected_numerical_features = selected_features,\n",
    "    scaler = scaler,\n",
    "    encoded_column_names = encoded_cols\n",
    ")\n",
    "\n",
    "predictions_ridge = ridge_model_final.predict(X_test_scaled)\n",
    "y_pred_ridge = clip_predictions(predictions_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031ae2e",
   "metadata": {},
   "source": [
    "blah blah blah\n",
    "\n",
    "## Training the Test Set: LightGBM Model\n",
    "\n",
    "blah blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_for_lightgbm(X_test, categorical_cols, numerical_cols, label_encoders):\n",
    "    X_test_processed = X_test.copy()\n",
    "    \n",
    "    # handle missing values\n",
    "    for col in numerical_cols:\n",
    "        if col in X_test_processed.columns:\n",
    "            median_val = X_test_processed[col].median()\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(median_val)\n",
    "    \n",
    "    # Label encode using training encoders\n",
    "    for col in categorical_cols:\n",
    "        if col in X_test_processed.columns:\n",
    "            le = label_encoders.get(col)\n",
    "            if le:\n",
    "                mode_val = X_test_processed[col].mode()[0] if not X_test_processed[col].mode().empty else 'Unknown'\n",
    "                X_test_processed[col] = X_test_processed[col].fillna(mode_val)\n",
    "\n",
    "                train_categories = set(le.classes_)\n",
    "                X_test_processed[col] = X_test_processed[col].map(\n",
    "                    lambda x: le.transform([x])[0] if x in train_categories else 0\n",
    "                ).astype(int)\n",
    "            else:\n",
    "                X_test_processed[col] = 0  # failsafe for unexpected cases\n",
    "    \n",
    "    return X_test_processed\n",
    "\n",
    "X_test_lgb = preprocess_test_for_lightgbm(\n",
    "    X_test_engineered,\n",
    "    categorical_cols = categorical_cols_eng,\n",
    "    numerical_cols = numerical_cols_eng,\n",
    "    label_encoders = lgb_encoders\n",
    ")\n",
    "\n",
    "predictions_lgb = final_lgb_model.predict(X_test_lgb)\n",
    "y_pred_lgb = clip_predictions(predictions_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4da5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ensemble = 0.5 * y_pred_ridge + 0.5 * y_pred_lgb\n",
    "y_pred_final = np.round(y_pred_ensemble).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'SII': y_pred_final\n",
    "})\n",
    "\n",
    "display(submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
